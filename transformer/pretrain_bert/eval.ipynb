{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c49d474-e2a0-49e1-bef2-4910535d2bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 随机MASK测试\n",
    "def random_mask_test(model, tokenizer, text_corpus, num_samples=1000):\n",
    "    # 从语料库中随机选择num_samples条文本\n",
    "    samples = random.sample(text_corpus, num_samples)\n",
    "\n",
    "    # 遍历每条文本进行随机MASK测试\n",
    "    total_correct = 0\n",
    "    total_words = 0\n",
    "    for sample in samples:\n",
    "        # 随机选择一些词进行MASK\n",
    "        tokens = tokenizer.tokenize(sample)\n",
    "        num_tokens = len(tokens)\n",
    "        masked_indices = random.sample(range(num_tokens), min(5, num_tokens))  # 最多MASK 5个词\n",
    "        for index in masked_indices:\n",
    "            masked_token = tokens[index]\n",
    "            tokens[index] = '[MASK]'\n",
    "            masked_text = ' '.join(tokens)\n",
    "            \n",
    "            # 使用模型预测MASK位置的词\n",
    "            input_ids = tokenizer.encode(masked_text, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids)\n",
    "                predictions = outputs[0].argmax(dim=-1).squeeze().tolist()\n",
    "            \n",
    "            # 获取预测结果并计算正确预测的数量\n",
    "            predicted_token = tokenizer.decode([predictions[index]])\n",
    "            if predicted_token == masked_token:\n",
    "                total_correct += 1\n",
    "            total_words += 1\n",
    "    \n",
    "    # 计算准确率\n",
    "    accuracy = total_correct / total_words\n",
    "    return accuracy\n",
    "\n",
    "# 在模型上执行随机MASK测试并计算准确率\n",
    "def evaluate(model, tokenizer, text_corpus, num_samples=1000):\n",
    "    model.eval()\n",
    "    accuracy = random_mask_test(model, tokenizer, text_corpus, num_samples)\n",
    "    print(f\"Random MASK test accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# # 使用示例\n",
    "# # model 和 tokenizer 是预训练的RoBERTa模型和tokenizer\n",
    "# # text_corpus 是语料库，即文本语料文件中的所有文本\n",
    "# evaluate(model, tokenizer, text_corpus, num_samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b487b2d-8b8d-43c7-a421-ecfa94e1f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(file_path):\n",
    "    corpus = []\n",
    "    with open(file_path) as f:\n",
    "        corpus = f.readlines()\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b519788-9012-42f4-9321-eb0330a55d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/project/aigc/llm/learning/transformer/pretrain_bert\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e752e959-b9fe-4ef4-9e9f-9d1847431616",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = load_corpus('/data2/project/aigc/llm/learning/transformer/pretrain_bert/all_output.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ae5536-752f-480a-9767-58d57faae9ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaTokenizer, RobertaForMaskedLM, AdamW\n\u001b[1;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, AdamW\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1969cc7-0a8e-40a2-898b-ce5615bd0765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: transformers in /home/nakai/miniconda3/envs/lmf/lib/python3.10/site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in /home/nakai/.local/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/nakai/.local/lib/python3.10/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nakai/.local/lib/python3.10/site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nakai/.local/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nakai/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nakai/.local/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /home/nakai/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/nakai/miniconda3/envs/lmf/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/nakai/.local/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nakai/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nakai/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nakai/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nakai/.local/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nakai/.local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nakai/miniconda3/envs/lmf/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nakai/.local/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8af14a0b-a85b-4f02-8e72-11aa621f2dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bb98d3d-e42b-4729-894f-688e394e27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = random.sample(text_corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61438932-f238-4776-a36f-c84e96a5bc3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize(samples[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c402cb5b-cd03-4cf6-800e-c4bdc2ad4a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
